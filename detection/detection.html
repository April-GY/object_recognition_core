

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Detection &mdash; object_recognition_core</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/breathe.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="object_recognition_core" href="../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="../index.html">object_recognition_core</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/or.svg" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Detection</a><ul>
<li><a class="reference internal" href="#use">Use</a></li>
<li><a class="reference internal" href="#command-line-interface">Command Line Interface</a></li>
<li><a class="reference internal" href="#configuration-file">Configuration File</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/detection/detection.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="detection">
<span id="id1"></span><h1><a class="toc-backref" href="#id2">Detection</a><a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#detection" id="id2">Detection</a><ul>
<li><a class="reference internal" href="#use" id="id3">Use</a></li>
<li><a class="reference internal" href="#command-line-interface" id="id4">Command Line Interface</a></li>
<li><a class="reference internal" href="#configuration-file" id="id5">Configuration File</a></li>
</ul>
</li>
</ul>
</div>
<p>Using the different trained objects, we can now detect them.</p>
<div class="section" id="use">
<h2><a class="toc-backref" href="#id3">Use</a><a class="headerlink" href="#use" title="Permalink to this headline">¶</a></h2>
<div class="container">
<button class="toggleable_button label_Non-ROS" onclick="
function toggle(label) {
  $('.toggleable_button').css({border: '2px outset', 'border-radius': '4px'});
  $('.toggleable_button.label_' + label).css({border: '2px inset', 'border-radius': '4px'});
  $('.toggleable_div').css('display', 'none');
  $('.toggleable_div.label_' + label).css('display', 'block');
};
toggle('Non-ROS')">Non-ROS</button>
<script>

function toggle(label) {
  $('.toggleable_button').css({border: '2px outset', 'border-radius': '4px'});
  $('.toggleable_button.label_' + label).css({border: '2px inset', 'border-radius': '4px'});
  $('.toggleable_div').css('display', 'none');
  $('.toggleable_div.label_' + label).css('display', 'block');
};

$(document).ready(function() {
  var classList =$('.toggleable_button').attr('class').split(/\s+/);
  $.each( classList, function(index, item){
    if (item.substring(0, 5) === 'label') {
      toggle(item.substring(6));
    };
  });
});
</script>
<button class="toggleable_button label_ROS" onclick="
function toggle(label) {
  $('.toggleable_button').css({border: '2px outset', 'border-radius': '4px'});
  $('.toggleable_button.label_' + label).css({border: '2px inset', 'border-radius': '4px'});
  $('.toggleable_div').css('display', 'none');
  $('.toggleable_div.label_' + label).css('display', 'block');
};
toggle('ROS')">ROS</button>
<script>

function toggle(label) {
  $('.toggleable_button').css({border: '2px outset', 'border-radius': '4px'});
  $('.toggleable_button.label_' + label).css({border: '2px inset', 'border-radius': '4px'});
  $('.toggleable_div').css('display', 'none');
  $('.toggleable_div.label_' + label).css('display', 'block');
};

$(document).ready(function() {
  var classList =$('.toggleable_button').attr('class').split(/\s+/);
  $.each( classList, function(index, item){
    if (item.substring(0, 5) === 'label') {
      toggle(item.substring(6));
    };
  });
});
</script>
</div>
<div class="container">
<div class="toggleable_div label_Non-ROS"><p>Just run the detection.py script in /apps. This will run continuously on the input image/point cloud.</p>
<div class="highlight-sh"><div class="highlight"><pre>./apps/detection -c config_detection.sample
</pre></div>
</div>
<p>The server requires a configuration file through the <tt class="docutils literal"><span class="pre">-c</span></tt> option.</p>
</div></div>
<div class="container">
<div class="toggleable_div label_ROS"><p>If you want continuous detection, you can just run the detection script:</p>
<div class="highlight-sh"><div class="highlight"><pre>rosrun object_recognition_core detection -c config_detection.sample
</pre></div>
</div>
<p>Then again, there is also an actionlib server as detailed on <a class="reference external" href="http://wg-perception.github.com/object_recognition_ros/server.html#actionlib" title="(in object_recognition_ros v)"><em class="xref std std-ref">actionlib server</em></a>:</p>
<div class="highlight-sh"><div class="highlight"><pre>rosrun object_recognition_ros server -c config_detection.sample
</pre></div>
</div>
<p>This will start a server with a given configuration file.
If you want to test the server, just execute the client once:</p>
<div class="highlight-sh"><div class="highlight"><pre>rosrun object_recognition_ros client
</pre></div>
</div>
<p>You can also use roslaunch if you want traditional actionlib support. There is a <tt class="docutils literal"><span class="pre">config_file</span></tt> argument
that can help you choose different pipelines:</p>
<div class="highlight-sh"><div class="highlight"><pre>roslaunch object_recognition_ros server.robot.launch
</pre></div>
</div>
</div></div>
<p>A typical command line session might look like:</p>
<div class="highlight-ectosh"><div class="highlight"><pre><span class="mf">%</span><span class="gd"> apps/detection -c config_detection.sample</span>
[ INFO] [1317692023.717854617]: Initialized ros. node_name: /ecto_node_1317692023710501315
Threadpool executing [unlimited] ticks in 5 threads.
[ INFO] [1317692024.254588151]: Subscribed to topic:/camera/rgb/camera_info with queue size of 0
[ INFO] [1317692024.255467268]: Subscribed to topic:/camera/depth_registered/camera_info with queue size of 0
[ INFO] [1317692024.256186358]: Subscribed to topic:/camera/depth_registered/image with queue size of 0
[ INFO] [1317692024.256863212]: Subscribed to topic:/camera/rgb/image_color with queue size of 0
model_id: e2449bdc43fd6d9dd646fcbcd012daaa
span: 0.433393 meters
1
***Starting object: 0
* starting RANSAC
 added : 1
 added : 0
* n inliers: 1824
[-0.056509789, 0.99800211, 0.028263446;
  0.94346958, 0.062639669, -0.32548648;
  -0.32660651, 0.0082725696, -0.94512439]
[-0.32655218; 0.03684178; 0.85040951]
********************* found 1poses
[ INFO] [1317692117.187226953]: publishing to topic:/object_ids
[ INFO] [1317692117.188155476]: publishing to topic:/poses
</pre></div>
</div>
</div>
<div class="section" id="command-line-interface">
<h2><a class="toc-backref" href="#id4">Command Line Interface</a><a class="headerlink" href="#command-line-interface" title="Permalink to this headline">¶</a></h2>
<div class="highlight-text"><div class="highlight"><pre>usage: detection [-h] [-c CONFIG_FILE] [--visualize] [--niter ITERATIONS]
                 [--shell] [--gui] [--logfile LOGFILE] [--graphviz]
                 [--dotfile DOTFILE] [--stats]

optional arguments:
  -h, --help            show this help message and exit
  -c CONFIG_FILE, --config_file CONFIG_FILE
                        Config file
  --visualize           If set and the pipeline supports it, it will display
                        some windows with temporary results

Ecto runtime parameters:
  --niter ITERATIONS    Run the graph for niter iterations. 0 means run until
                        stopped by a cell or external forces. (default: 0)
  --shell               &#39;Bring up an ipython prompt, and execute
                        asynchronously.(default: False)
  --gui                 Bring up a gui to help execute the plasm.
  --logfile LOGFILE     Log to the given file, use tail -f LOGFILE to see the
                        live output. May be useful in combination with --shell
  --graphviz            Show the graphviz of the plasm. (default: False)
  --dotfile DOTFILE     Output a graph in dot format to the given file. If no
                        file is given, no output will be generated. (default:
                        )
  --stats               Show the runtime statistics of the plasm.
</pre></div>
</div>
</div>
<div class="section" id="configuration-file">
<h2><a class="toc-backref" href="#id5">Configuration File</a><a class="headerlink" href="#configuration-file" title="Permalink to this headline">¶</a></h2>
<p>The configuration file is where you define your graph and with the current ORK, you can choose any of the following sources:</p>
<div class="highlight-text"><div class="highlight"><pre>source_0:
   type: RosKinect
   module: object_recognition_ros.io.source.ros_kinect
   parameters:
      #  If the cropper cell is enabled
      crop_enabled: True
      #  The ROS topic for the depth camera info.
      depth_camera_info: /camera/depth_registered/camera_info
      #  The ROS topic for the depth image.
      depth_image_topic: /camera/depth_registered/image_raw
      #  The ROS topic for the RGB camera info.
      rgb_camera_info: /camera/rgb/camera_info
      #  The ROS topic for the RGB image.
      rgb_image_topic: /camera/rgb/image_color
      #  The maximum x value (in the camera reference frame)
      x_max: 3.40282346639e+38
      #  The minimum x value (in the camera reference frame)
      x_min: -3.40282346639e+38
      #  The maximum y value (in the camera reference frame)
      y_max: 3.40282346639e+38
      #  The minimum y value (in the camera reference frame)
      y_min: -3.40282346639e+38
      #  The maximum z value (in the camera reference frame)
      z_max: 3.40282346639e+38
      #  The minimum z value (in the camera reference frame)
      z_min: -3.40282346639e+38

source_1:
   type: BagReader
   module: object_recognition_ros.io.source.bag_reader
   parameters:
      #  The bag file name.
      bag: data.bag
      #  If the cropper cell is enabled
      crop_enabled: True
      #  The ROS topic for the depth camera info.
      depth_camera_info: /camera/depth_registered/camera_info
      #  The ROS topic for the depth image.
      depth_image_topic: /camera/depth_registered/image_raw
      #  The ROS topic for the RGB camera info.
      rgb_camera_info: /camera/rgb/camera_info
      #  The ROS topic for the RGB image.
      rgb_image_topic: /camera/rgb/image_color
      #  The maximum x value (in the camera reference frame)
      x_max: 3.40282346639e+38
      #  The minimum x value (in the camera reference frame)
      x_min: -3.40282346639e+38
      #  The maximum y value (in the camera reference frame)
      y_max: 3.40282346639e+38
      #  The minimum y value (in the camera reference frame)
      y_min: -3.40282346639e+38
      #  The maximum z value (in the camera reference frame)
      z_max: 3.40282346639e+38
      #  The minimum z value (in the camera reference frame)
      z_min: -3.40282346639e+38

source_2:
   type: OpenNI
   module: object_recognition_core.io.source
   parameters:
      #  The number of frames per second for the depth image: [ecto_openni.FpsMode.FPS_60,
      # ecto_openni.FpsMode.FPS_30, ecto_openni.FpsMode.FPS_15]
      depth_fps: None
      #  The resolution for the depth image: [ecto_openni.ResolutionMode.QQVGA_RES,
      # ecto_openni.ResolutionMode.CGA_RES, ecto_openni.ResolutionMode.QVGA_RES,
      # ecto_openni.ResolutionMode.VGA_RES, ecto_openni.ResolutionMode.XGA_RES,
      # ecto_openni.ResolutionMode.HD720P_RES, ecto_openni.ResolutionMode.SXGA_RES,
      # ecto_openni.ResolutionMode.UXGA_RES, ecto_openni.ResolutionMode.HD1080P_RES]
      depth_mode: None
      #  The number of frames per second for the RGB image: [ecto_openni.FpsMode.FPS_60,
      # ecto_openni.FpsMode.FPS_30, ecto_openni.FpsMode.FPS_15]
      image_fps: None
      #  The resolution for the RGB image: [ecto_openni.ResolutionMode.QQVGA_RES,
      # ecto_openni.ResolutionMode.CGA_RES, ecto_openni.ResolutionMode.QVGA_RES,
      # ecto_openni.ResolutionMode.VGA_RES, ecto_openni.ResolutionMode.XGA_RES,
      # ecto_openni.ResolutionMode.HD720P_RES, ecto_openni.ResolutionMode.SXGA_RES,
      # ecto_openni.ResolutionMode.UXGA_RES, ecto_openni.ResolutionMode.HD1080P_RES]
      image_mode: None
      #  The stream mode: [ecto_openni.StreamMode.IR, ecto_openni.StreamMode.DEPTH,
      # ecto_openni.StreamMode.DEPTH_IR, ecto_openni.StreamMode.RGB,
      # ecto_openni.StreamMode.DEPTH_RGB]
      stream_mode: None
</pre></div>
</div>
<p>any of the following sinks:</p>
<div class="highlight-text"><div class="highlight"><pre>sink_0:
   type: GuessCsvWriter
   module: object_recognition_core.io.sink
   parameters:
      #  The run number
      run_number: 0
      #  The name of the team to consider
      team_name: 

sink_1:
   type: Publisher
   module: object_recognition_by_parts.publisher
   parameters:

sink_2:
   type: TablePublisher
   module: object_recognition_tabletop.table_publisher
   parameters:
      #  Determines if the topics will be latched.
      latched: True
      #  The ROS topic to use for the markers of the clusters.
      marker_array_clusters: marker_array_clusters
      #  The ROS topic to use for the table message.
      marker_hull_topic: marker_table_hulls
      #  The ROS topic to use for the table message.
      marker_origin_topic: marker_table_origins
      #  The ROS topic to use for the table message.
      marker_table_topic: marker_tables
      #  The array of found tables.
      table_array: table_array

sink_3:
   type: Publisher
   module: object_recognition_ros.io.sink.publisher
   parameters:
      #  The DB parameters
      db_params: &lt;object_recognition_core.boost.interface.ObjectDbParameters object at 0x3311f18&gt;
      #  Determines if the topics will be latched.
      latched: True
      #  The ROS topic to use for the marker array.
      markers_topic: markers
      #  The ROS topic to use for the object meta info string
      object_ids_topic: object_ids
      #  The ROS topic to use for the pose array.
      pose_topic: poses
      #  Sets whether the point cloud clusters have to be published or not
      publish_clusters: True
      #  The ROS topic to use for the recognized object
      recognized_object_array_topic: recognized_object_array
</pre></div>
</div>
<p>or the following pipelines:</p>
<div class="highlight-text"><div class="highlight"><pre>detection_pipeline_0:
   type: LinemodDetector
   module: object_recognition_linemod.detector
   parameters:
      #  The DB configuration parameters as a JSON string
      json_db: 
      #  A set of object ids as a JSON string: &#39;[&quot;1576f162347dbe1f95bd675d3c00ec6a&quot;]&#39; or &#39;all&#39;
      json_object_ids: all
      #  The method the models were computed with
      method: LINEMOD
      #  Matching threshold, as a percentage
      threshold: 93.0
      #  If True, visualize the output.
      visualize: False

detection_pipeline_1:
   type: TabletopObjectDetector
   module: object_recognition_tabletop.detector
   parameters:
      #  The DB parameters
      json_db: 
      #  The DB id of the objects to load in the household database.
      json_object_ids: 
      #  The object_ids set as defined by the household object database.
      tabletop_object_ids: REDUCED_MODEL_SET

detection_pipeline_2:
   type: PartsBasedDetector
   module: object_recognition_by_parts.detector
   parameters:
      #  The DB configuration parameters as a JSON string
      json_db: 
      #  A set of object ids as a JSON string: &#39;[&quot;1576f162347dbe1f95bd675d3c00ec6a&quot;]&#39; or &#39;all&#39;
      json_object_ids: all
      #  The maximum overlap allowed between object detections
      max_overlap: 0.10000000149
      #  The method the models were computed with
      method: PartsBased
      #  The path to the model file
      model_file: 
      #  The cell should remove planes from the scene before the cluster extraction
      remove_planes: False
      #  Visualize results
      visualize: False

detection_pipeline_3:
   type: TransparentObjectsDetector
   module: object_recognition_transparent_objects.detector
   parameters:
      #  The DB configuration parameters as a JSON string
      json_db: 
      #  A set of object ids as a JSON string: &#39;[&quot;1576f162347dbe1f95bd675d3c00ec6a&quot;]&#39; or &#39;all&#39;
      json_object_ids: all
      #  The method the models were computed with
      method: TransparentObjects
      #  The DB parameters
      object_db: None
      #  The filename of the registration mask.
      registrationMaskFilename: 
      #  Visualize results
      visualize: False

detection_pipeline_4:
   type: TabletopTableDetector
   module: object_recognition_tabletop.detector
   parameters:
      #  The maximum distance between a point and the cluster it belongs to.
      cluster_distance: 0.019999999553
      #  Min number of points for a cluster
      min_cluster_size: 300
      #  The minimum number of points deemed necessary to find a table.
      min_table_size: 10000
      #  The distance used as a threshold when finding a plane
      plane_threshold: 0.019999999553
      #  The distance used when clustering a plane
      table_cluster_tolerance: 0.20000000298
      #  The vertical frame id
      vertical_frame_id: /map
      #  The amount to keep in the z direction (meters) relative to the coordinate frame defined by
      # the pose.
      z_crop: 0.5
      #  The amount to crop above the plane, in meters.
      z_min: 0.00749999983236

detection_pipeline_5:
   type: TodDetector
   module: object_recognition_tod.detector
   parameters:
      #  The DB to get data from as a JSON string
      json_db: {}
      #  Parameters for the descriptor as a JSON string. It should have the format:
      # &quot;{&quot;type&quot;:&quot;ORB/SIFT whatever&quot;, &quot;module&quot;:&quot;where_it_is&quot;, &quot;param_1&quot;:val1, ....}
      json_descriptor_params: {&quot;type&quot;: &quot;ORB&quot;, &quot;module&quot;: &quot;ecto_opencv.features2d&quot;}
      #  Parameters for the feature as a JSON string. It should have the format: &quot;{&quot;type&quot;:&quot;ORB/SIFT
      # whatever&quot;, &quot;module&quot;:&quot;where_it_is&quot;, &quot;param_1&quot;:val1, ....}
      json_feature_params: {&quot;type&quot;: &quot;ORB&quot;, &quot;module&quot;: &quot;ecto_opencv.features2d&quot;}
      #  The ids of the objects to find as a JSON list or the keyword &quot;all&quot;.
      json_object_ids: all
      #  Minimum number of inliers
      min_inliers: 15
      #  Number of RANSAC iterations.
      n_ransac_iterations: 1000
      #  The search parameters as a JSON string
      search: {}
      #  The error (in meters) from the Kinect
      sensor_error: 0.00999999977648
      #  If true, some windows pop up to see the progress
      visualize: False
</pre></div>
</div>
<p>More of any of those can be added by the user obviously</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li><a href="../index.html">object_recognition_core</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013,  Willow Garage, Inc.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>